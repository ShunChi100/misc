{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 Lecture 2\n",
    "\n",
    "Learning goals:\n",
    "\n",
    "- why do we need gradient descent? Can't we use the normal equations for everything?\n",
    "- what is the step size? what role does it play?\n",
    "- how do I implement gradient descent?\n",
    "- what's the difference between the data space and the parameter space?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: least squares with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data\n",
    "x = np.random.randn(40)\n",
    "y = 10*x\n",
    "# add random outliers\n",
    "Noutliers = 3\n",
    "y[:Noutliers] = -100*(x[:Noutliers]+np.random.randn(Noutliers))\n",
    "\n",
    "# plot the points\n",
    "plt.plot(x,y,'.')\n",
    "\n",
    "X = x[:,None] # reshape for sklearn\n",
    "\n",
    "# fit a linear regression model\n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "lr.fit(X,y)\n",
    "plt.plot(x, lr.predict(X))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### use of the gradient: optimization\n",
    "\n",
    "The gradient gives us the direction of fastest increase.\n",
    "\n",
    "The (negative) gradient gives us a direction of travel (fastest decrease), if we want to minimize a function!\n",
    "\n",
    "If we are trying to minimize $f(\\mathbf{w})$, then gradient descent works by starting with some initial guess $\\mathbf{w}_0$ and then updating with\n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\alpha_t\\, \\nabla f(\\mathbf{w}_{t})$$\n",
    "\n",
    "- Note: gradient descent is an algorithm for minimization. The corresponding method for maximization is called gradient ascent.\n",
    "- The step size should be small to guarantee that the loss goes down. If it's too big the loss could go up.\n",
    "- (optional note): physicists like me tend to think about _units_. Note that $f$ and $\\nabla f$ have different units, which means the learning rate has units. That seems weird, since we'd hope to pick the learning rate irrespective of the scaling of our probelm. If this bothers you, see [this blog post](http://timvieira.github.io/blog/post/2016/05/27/dimensional-analysis-of-gradient-ascent/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use this to minimize losses like the squared error (above) and robust losses (coming soon):\n",
    "\n",
    "\n",
    "$$ f(\\mathbf{w}) = \\sum_{n=1}^N\\left(\\mathbf{w}^\\top\\mathbf{x_n}-y_n\\right)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### magnitude vs. direction\n",
    "\n",
    "In the above we are making use of the magnitude of the gradient, not just its direction. This actually makes sense. When the magnitude is small, we are in a flatter area and want to take smaller steps. \n",
    "\n",
    "However, some methods exist that just use the direction. For example given a direction you can do a [line search](https://en.wikipedia.org/wiki/Line_search).\n",
    "\n",
    "#### picking the learning rate\n",
    "\n",
    "In general picking $\\alpha$ is a pain. There is theory on this regarding convergence guarantees and convergence rates. The learning rate is sometimes decreased over time. Fancier methods pick the learning rate adaptively.\n",
    "\n",
    "#### termination conditions\n",
    "\n",
    "We can stop when $||\\nabla f||$ is sufficiently small (because this indicates we're at a local minimum), or when a specified maximum number of iterations are reached (because we want to limit the computational expense).\n",
    "\n",
    "#### why gradient descent\n",
    "\n",
    "Question: why do we want to use the gradient? \n",
    "\n",
    "- One way to think about it is that you get $d$ times more information than just the function value\n",
    "- Because it takes $d+1$ evaluations to estimate the gradient (we'll discuss this in lecture 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares via normal equations vs. gradient descent:\n",
    "\n",
    "- Normal equations cost $\\mathcal{O}(nd^2 + d^3)$.\n",
    "  - forming $X^TX$ costs $\\mathcal{O}(nd^2)$ and solving a $d\\times d$ linear system costs $\\mathcal{O}(d^3)$\n",
    "- Gradient descent costs $\\mathcal{O}(ndt)$ to run for $t$ iterations.\n",
    "  - computing $\\nabla f(\\mathbf{w})=X^TXw-X^Ty$ only costs $\\mathcal{O}(nd)$ \n",
    "- Gradient descent can be faster when $d$ is very large\n",
    "  - well, this is just big-O, we don't know the coeffcients\n",
    "  - matrix operations are fast. iteration is slow, especially in Python\n",
    "  - gradient descent is usually not the way to go with OLS, but we'll see its usefulness below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments: does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "d = 10\n",
    "n = 1000\n",
    "\n",
    "X = npr.randn(n,d)\n",
    "y = npr.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1: sklearn's `LinearRegression` in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn least squares\n",
    "\n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "lr.fit(X,y)\n",
    "print(\"Intercept: %f\" % lr.intercept_)\n",
    "print(\"Weights: %s\" % lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2: normal equations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a feature of all 1's for intercept \n",
    "X = np.append(np.ones(X.shape[0])[:,None], X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.linalg.solve(X.T@X,X.T@y)\n",
    "\n",
    "print(\"Intercept: %f\" % weights[0])\n",
    "print(\"Weights: %s\" % weights[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 3: gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.00001\n",
    "\n",
    "grad_f = lambda w: X.T@(X@w) - X.T@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.zeros(X.shape[1])\n",
    "\n",
    "while True:\n",
    "    g = grad_f(w)\n",
    "    \n",
    "    if np.linalg.norm(g) < 0.001: # is size of gradient < 0.001?\n",
    "        break\n",
    "    \n",
    "    w -= alpha*g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept: %f\" % w[0])\n",
    "print(\"Weights: %s\" % w[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, you can see that all three methods generate the same weights. This is comforting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why gradient descent?\n",
    "\n",
    "If we can solve least squares in closed form with linear algebra, why do we want gradient descent?\n",
    "\n",
    "Answer: the fact that we can write the solution in closed form as a system of linear equations is amazing! (Think about it.) This is a very special case. For almost any other loss function, we can't do this.\n",
    "\n",
    "For example with gradient descent we can minimize the absolute value objective\n",
    "\n",
    "$$ f(\\mathbf{w}) = \\sum_{n=1}^N\\left|\\mathbf{w}^\\top\\mathbf{x_n}-y_n\\right| $$\n",
    "\n",
    "with gradient descent. This is the sum of absolute values instead of the sum of squares. This will hopefully give us robust regression because big distances aren't amplified in the objective (let's dwell on this reasoning for a bit).\n",
    "\n",
    "**Note:** You might point out that this function is non-differentiable. See the bonus material at the bottom for more info. But in general **it is not OK to ignore** the non-smoothness and it needs to be addressed, either by using a different optimization algorithm or by smoothing the loss function. \n",
    "\n",
    "Experiments (this time we add outliers to mess up least squares):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data\n",
    "x = np.random.randn(40)\n",
    "y = 10*x\n",
    "# add random outliers\n",
    "Noutliers = 3\n",
    "y[:Noutliers] = -100*(x[:Noutliers]+np.random.randn(Noutliers))\n",
    "\n",
    "# plot the points\n",
    "plt.plot(x,y,'.')\n",
    "\n",
    "X = x[:,None] # reshape for sklearn\n",
    "\n",
    "# fit a linear regression model\n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "lr.fit(X,y)\n",
    "plt.plot(x, lr.predict(X))\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"w = %f\" % lr.coef_)\n",
    "print(\"b = %f\" % lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression in one dimension our L1 objective becomes\n",
    "\n",
    "$$f(w,b)=\\displaystyle \\sum_{n=1}^N\\left|w x_n +b - y_n\\right| $$\n",
    "\n",
    "The gradient is\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial w} = \\sum_{n=1}^N  x_n \\textrm{sign}\\left(w x_n +b - y_n\\right)$$ \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial b} = \\sum_{n=1}^N  \\textrm{sign}\\left(w x_n +b - y_n\\right)$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0\n",
    "b = 0\n",
    "alpha = 1\n",
    "\n",
    "Nsteps = 1000\n",
    "\n",
    "for t in range(1,Nsteps):\n",
    "    dLdw = np.sum(x*np.sign(w*x+b-y))\n",
    "    dLdb = np.sum(np.sign(w*x+b-y))\n",
    "    w -= alpha/t*dLdw\n",
    "    b -= alpha/t*dLdb\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(x,y,'.')\n",
    "plt.plot(x,w*x+b)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "print(\"w = %f\" % w)\n",
    "print(\"b = %f\" % b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, \"do not try this at home\" because of the non-smoothness. Just doing it now to keep the example as simple as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent is not just for robust regression\n",
    "\n",
    "We can use it for a large class of models, like logistic regression (DSCI 562) and deep learning (later in this course). With some caveats, **you are now empowered to minimize things!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent is not the only game in town\n",
    "\n",
    "There are many optimization algorithms out there. Gradient descent is not applicable in all contexts and it is certainly not the best method in all contexts! We focus on it because:\n",
    "\n",
    "- it is relatively simple to understand and implement.\n",
    "- the time complexity is linear in $d$ per iteration.\n",
    "- we can generalize it to _stochastic gradient descent_, which is coming later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The different decisions we make\n",
    "\n",
    "We should try to keep separate (in our minds) the following choices:\n",
    "\n",
    "- the model (e.g. linear)\n",
    "- the loss (e.g. squared error)\n",
    "- the optimization method (e.g. gradient descent)\n",
    "\n",
    "We can (roughly) pick these independently of each other. Note quite because the choice of optimization method may depend on the choice of model/loss: e.g. we can use the normal equations as our optimization method if we're doing least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What space am I in?\n",
    "\n",
    "A key skill is being able to move around (conceptually) between $x$ (data) space and $w$ (parameter) space. The above plots are in $y$ vs. $x$ space. Here's what the loss looks like in parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_fun, X, y): # reading/understanding this plotting code is optional\n",
    "    m = 100\n",
    "    w_lin = np.linspace(-20.0, 20.0, m)\n",
    "    b_lin = np.linspace(-20.0, 20.0, m)\n",
    "    w_grid, b_grid = np.meshgrid(w_lin, b_lin)\n",
    "    w_flat = w_grid.flatten()\n",
    "    b_flat = b_grid.flatten()\n",
    "    \n",
    "    pred = w_flat[None]*X + b_flat[None]\n",
    "\n",
    "    loss = loss_fun(pred, y) \n",
    "    loss_grid = np.reshape(loss,[m,m])\n",
    "\n",
    "    plt.figure()\n",
    "    CS = plt.contour(w_grid, b_grid, loss_grid)\n",
    "    imin = np.argmin(loss_grid)\n",
    "    plt.plot(w_flat[imin], b_flat[imin], 'r*', markersize=15)\n",
    "    plt.xlabel('w')\n",
    "    plt.ylabel('b')\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.title('Loss in $w$-$b$ space')\n",
    "    plt.show()\n",
    "              \n",
    "squared_loss_fun = lambda pred, y: np.sum((pred-y[:,None])**2,axis=0)\n",
    "plot_loss(squared_loss_fun, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abs_loss_fun = lambda pred, y: np.sum(np.abs(pred-y[:,None]),axis=0)\n",
    "\n",
    "plot_loss(abs_loss_fun, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key intuition: every point in weight space corressponds to a _model_ (in this case a line) in input-output space.\n",
    "\n",
    "- In general, doing supervised learning in $d$ dimensions, the data space will be mapping from $d$ dimensions to $1$ dimensional. \n",
    "- For linear models, the number of parameters is $d+1$ ($d$ weights plus an intercept), but this isn't necessarily the case for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few iterations of GD showing the loss space and line space\n",
    "x = np.random.randn(40)\n",
    "y = 3*x-9\n",
    "X = x[:,None]\n",
    "\n",
    "w = 0\n",
    "b = 0\n",
    "alpha = 0.01\n",
    "\n",
    "Nsteps = 5\n",
    "\n",
    "def make_plot_pairs(w,b,t,titles=False):\n",
    "    plt.subplot(Nsteps+1,2,2*t+1)    \n",
    "    plt.plot(x,y,'.')\n",
    "    plt.plot(x,w*x+b)\n",
    "    plt.ylabel('iteration %d' % t)\n",
    "    if titles:\n",
    "        plt.title(\"Data space\")\n",
    "    \n",
    "    plt.subplot(Nsteps+1,2,2*t+2)\n",
    "    m = 100\n",
    "    w_lin = np.linspace(-10.0, 10.0, m)\n",
    "    b_lin = np.linspace(-10.0, 10.0, m)\n",
    "    w_grid, b_grid = np.meshgrid(w_lin, b_lin)\n",
    "    w_flat = w_grid.flatten()\n",
    "    b_flat = b_grid.flatten()\n",
    "    pred = w_flat[None]*X + b_flat[None]\n",
    "    loss = squared_loss_fun(pred, y) \n",
    "    loss_grid = np.reshape(loss,[m,m])\n",
    "    imin = np.argmin(loss_grid)\n",
    "    plt.plot(w_flat[imin], b_flat[imin], 'r*', markersize=15)\n",
    "    CS = plt.contour(w_grid, b_grid, loss_grid)\n",
    "    plt.plot(w, b, 'b*', markersize=15)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    if titles:\n",
    "        plt.title(\"Parameter space\")\n",
    "\n",
    "plt.figure(figsize=(8, 20))\n",
    "make_plot_pairs(w,b,0,True)    \n",
    "for t in range(Nsteps):\n",
    "    dLdw = np.sum(x*(w*x+b-y))\n",
    "    dLdb = np.sum(w*x+b-y)\n",
    "    w -= alpha*dLdw\n",
    "    b -= alpha*dLdb\n",
    "    \n",
    "    make_plot_pairs(w,b,t+1)\n",
    "\n",
    "    \n",
    "print(\"w = %f\" % w)\n",
    "print(\"b = %f\" % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Robust regression intuition\n",
    "\n",
    "The loss is the sum of the absolute vertical distances between the points and the line. Let's assume you have 3 outliers all on the same side of the line, which are dragging the line up/down. Imagine shifting the line in towards the outliers by some small amount $\\epsilon$. You have 3 points (the outliers) that are happier, meaning the loss goes down by $3\\epsilon$. But, supposing there are 40 non-outlier points, you've upset those 40 points, meaning the loss goes up by $40 \\epsilon$. So even moving the slightest bit upwards, or any amount for that matter, results in a worse (higher loss). That's why the robust fit is \"perfect\" rather than \"close\". You can think of it as voting and each point gets one vote; the outliers don't get more important votes than the non-outliers. So 40 beats 3. In real situations this reasoning doesn't apply exactly because the non-outliers will not lie perfectly on a line, and because you'll be in $d > 1$, but the general thought process applies.\n",
    "\n",
    "We can go through the same thought experiment with least squares. If we move up by $\\epsilon$ then the change in the loss function due to the outliers might be huge. For example if the outliers are currently 10 units away from the line and $\\epsilon=1$ then we go from a loss of $10^2=100$ to $9^2=81$, so the loss decreases by 19 units for each outlier! Whereas for the non-outliers the loss increases from $0^2$ to $1^2$, and so the net increase is 1 unit of loss per point. Thus 3 outliers can actually overrule 40 non-outliers and the line starts to move. Until it reaches that equilibrium where the loss is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) smooth approximations and Huber loss\n",
    "\n",
    "The absolute value loss is **non-smooth**. Although things above looked OK, in general it's not OK to naively ignore this issue (gradient descent will not asymptotically converge to the true solution, the above looks OK because we don't need a high accuracy solution). One way to avoid this issue is to use a **smooth approximation** to the loss. For example we can minimize the Huber objective\n",
    "\n",
    "$$ \\sum_{i=1}^n h \\left( y_i-w^\\top x_i\\right) $$\n",
    "\n",
    "where $$h(z) \\equiv \\begin{cases} \n",
    "      \\frac{1}{2} z^2 & \\textrm{if} \\; |z|\\leq 1 \\\\   |z|-\\frac12 & \\textrm{if} \\; |z|>1\n",
    "   \\end{cases} $$\n",
    "\n",
    "with gradient descent. This will hopefully give us robust regression because big distances aren't amplified in the objective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber = lambda z: 0.5*z**2*(np.abs(z)<=1) + (np.abs(z)-0.5)*(np.abs(z)>1)\n",
    "\n",
    "grid = np.linspace(-3,3,1000)\n",
    "plt.plot(grid, np.abs(grid), label=\"abs\")\n",
    "plt.plot(grid, huber(grid), 'r', label=\"huber\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression in one dimension our objective becomes\n",
    "\n",
    "$$f(w,b)=\\displaystyle \\sum_{i=1}^n h\\left(w x_i +b - y_i\\right) $$\n",
    "\n",
    "The gradient is\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial w} = \\sum_{i=1}^n  x_i h'\\left( w x_i +b - y_i\\right)$$ \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial b} = \\sum_{i=1}^n  h'\\left(w x_i +b - y_i\\right)$$ \n",
    "\n",
    "where $$h'(z) = \\begin{cases} \n",
    "       z & \\textrm{if} \\; |z|\\leq 1 \\\\   \\textrm{sign}(z) & \\textrm{if} \\; |z|>1\n",
    "   \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing this objective yields the robust fit, and in general is the right thing to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0\n",
    "b = 0\n",
    "alpha = 0.01\n",
    "\n",
    "Nsteps = 1000\n",
    "yhat = lambda x,w,b: w*x+b\n",
    "\n",
    "huber_deriv = lambda z: z*(np.abs(z)<=1) + np.sign(z)*(np.abs(z)>1)\n",
    "\n",
    "for t in range(Nsteps):\n",
    "    dLdw = -np.sum(x*huber_deriv(y-yhat(x,w,b)))\n",
    "    dLdb = -np.sum(huber_deriv(y-yhat(x,w,b)))\n",
    "    w -= alpha*dLdw\n",
    "    b -= alpha*dLdb\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(x,y,'.')\n",
    "plt.plot(x,w*x+b)\n",
    "plt.show()\n",
    "\n",
    "print(\"w = %f\" % w)\n",
    "print(\"b = %f\" % b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
